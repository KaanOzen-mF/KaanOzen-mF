{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Classification.</h2>\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this round, you will learn how to formulate and solve a **classification problem**. Recall that **regression problems** are machine learning problems that involve data points with a numeric label such as the grayscale level of a pixel. In contrast, **classification problems** arise from data points whose labels have only a finite number of different values. The most simple classification problem is a **binary classification problem** where the label can take on only two distinct values such as $y=0$ vs. $y=1$. For example, $y$=\"picture includes a pedestrian crossing\" vs. $y$=\"picture does not include pedestrian crossing\". The label $y$ of a data point indicates to which class (or category) the data point belongs. \n",
    "\n",
    "We consider two widely used methods for solving classification problems: **logistic regression** and **decision trees**. These two methods differ in the choice of hypothesis space, i.e., the set of predictor functions $h(\\mathbf{x})$ that map the features $\\mathbf{x}$ of a data point to a predicted label $\\hat{y}=h(\\mathbf{x})$, which is hopefully a good approximation of the true label $y$. \n",
    "\n",
    "We mainly consider binary classification problems with data points having labels from a set of size two such as $\\{0,1\\}$ or {\"image shows a crossing\", \"image shows no crossing\"}. However, we will also discuss a simple approach to upgrade any binary classification method to solve classification problems with more than two label values such as {\"image shows one crossing\", \"image shows more than one crossing\", \"image shows no crossing\"}. We refer to classification problems with more than two label values (or categories) as **multi-class classification problems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning goals\n",
    "\n",
    "After this round, you should  \n",
    "\n",
    "- be able to model \"real-world\" applications as classification problems by identifying features and labels. \n",
    "- be able to solve classification problems using logistic regression or decision trees. \n",
    "- be able to assess the reliability of classifications provided by logistic regression. \n",
    "- know about the differences between decision trees and logistic regression. \n",
    "- know how to extend binary classification methods to multi-class problems where labels can take on more than two different values. \n",
    "\n",
    "\n",
    " Reading Material \n",
    "\n",
    "* Chapter Chapter 3.6 & 3.10 of course book [Machine Learning: The basics.]\n",
    "\n",
    "\n",
    "\n",
    " Additional Material \n",
    "* [video-lecture](https://www.youtube.com/watch?v=-la3q9d7AKQ) of Prof. Andrew Ng on classification problems and logistic regression \n",
    "* [video-lecture](https://www.youtube.com/watch?v=ZvaELFv5IpM) of Prof. Andrew Ng on extending binary classification methods to multi-class problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Problem\n",
    "\n",
    "<img src=\"N5_Classification/CrossingDetection.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The city planners of Helsinki are regularly sending small airplanes to take high-resolution aerial photographs of different city areas. These aerial photographs are available via the open data service at https://kartta.hel.fi. It is important to monitor the condition of pedestrian crossings to determine if a renewal is necessary. To this end, we want to find those areas which contain a pedestrian crossing. \n",
    "\n",
    "In this exercise, you will learn how to use classification methods to determine if a particular area contains a pedestrian (zebra) crossing or not. We model this pedestrian crossing detection as a machine learning problem. The problem amounts to learn a predictor (or classifier) map $h(\\mathbf{x})$ which delivers a predicted label $\\hat{y} = h(\\mathbf{x})$ which indicates if a certain area contains a pedestrian crossing or not. The classification is based on numeric features $\\mathbf{x}=\\big(x_{1},\\ldots,x_{n}\\big)^{T}$ that are computed from an aerial photograph of the area in question.  \n",
    "\n",
    "We will solve this binary classification problem using two different classification methods: logistic regression and decision trees. These two methods differ in the choice of hypothesis space. Decision tree classifiers use a flow-chart representation of the predictor function. In contrast, logistic regression uses the hypothesis space of linear predictor functions, which is also used in linear regression (see Notebook 3 - Regression). \n",
    "\n",
    "The difference between logistic and linear regression is the set of label values: the real numbers for linear regression and a set of size two {0,1} for logistic regression. Another difference between linear and logistic regression is the loss function. While linear regression is based on minimizing the squared error loss, logistic regression minimizes the logistic loss function, which is presented later. \n",
    "\n",
    "As you might already know, most machine learning problems (and methods) consist of three components: \n",
    "\n",
    "* some **data** (a bunch of data points, each of which is characterized by features and labels) \n",
    "* a **hypothesis space** (consisting of a set of predictor functions from features to labels)\n",
    "* a **loss function** which is used to assess the quality of a particular predictor function \n",
    "\n",
    "In what follows, we will discuss particular choices for these three components to solve the pedestrian crossing detection problem.  \n",
    "\n",
    " The Data\n",
    "\n",
    "ML methods aim at finding a good predictor map (or classifier) $h(\\mathbf{x})$, which takes some features $\\mathbf{x}$ as input and outputs an estimate for the label $y$ of the data point (which represents an area). To measure the quality of a particular predictor $h(\\mathbf{x})$, we apply it on data points for which we know the true label values $y$. The basic principle of classification methods is to find (or learn) the best predictor function out of a set of computationally feasible functions (the hypothesis space). \n",
    "\n",
    "We have access to a data set consisting of $m=178$ data points $\\big(\\mathbf{x}^{(i)},y^{(i)}\\big)$ for $i=1,\\ldots,m$. Each data point represents a particular city area, and is characterized by several features $\\mathbf{x}^{(i)}=\\big(x^{(i)}_{1},\\ldots,x^{(i)}_{n}\\big)^{T}$ that are computed from an aerial photograph of that area. Each data point is also characterized by a label $y^{(i)}$ which has been found out by a city planner who manually inspected the areal photograph. \n",
    "\n",
    "We can use the labeled data to find a good predictor $h(\\mathbf{x})$. In contrast to regression problems, where the ouput $h(\\mathbf{x})$ of a predictor map is a (real) number, the predicted label $h(\\mathbf{x})$ in a classification problem is a discrete value representing a particular class of data points. In this case it is customary to use the term **classifier** for the predictor $h(\\mathbf{x})$.\n",
    "\n",
    "A good classifier $h(\\mathbf{x})$ should at least agree well with similar human judgment,\n",
    "\n",
    "\\begin{equation} \n",
    "\\underbrace{y^{(i)}}_{\\mbox{label by human}} \\approx \\underbrace{h(\\mathbf{x}^{(i)})}_{\\mbox{predicted label } \\hat{y}^{(i)}}  \\mbox{ for all } i =1,\\ldots,m. \n",
    "\\end{equation}\n",
    "\n",
    "To sum up, \n",
    "* The dataset contains information about $m=178$ areas in the city of Helsinki.  \n",
    "* For each area, a feature vector $\\mathbf{x}^{(i)}$ containing $n=5$ features has been determined. \n",
    "* For each area, a city planner determined the class $y^{(i)}$ which is either \n",
    "     * $y^{(i)} = 0$ (area has no pedestrian crossing)\n",
    "     * $y^{(i)}=1$ (area has one pedestrian crossing) \n",
    "     * $y^{(i)}=2$ (area has more than one pedestrian crossing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demoboundary'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    " Demo. Load Data.\n",
    "\n",
    "In the code below, we define a function `load_data` that loads the feature matrix $\\mathbf{X}$ and label vector $\\mathbf{y}$ of the zebra crossing dataset. The dataset has $m=178$ data points, and we load the five first features.\n",
    "    \n",
    "The parameter `binary_labels` defines whether we load three labels $\\mathbf{y}$ as defined above, or convert the problem into a binary classification task by defining new labels with $y^{(i)}=1$ for areas with no pedestrian crossings and $y^{(i)}=0$ for areas with one or more crossings. The default value is `binary_labels=False`, i.e, the function returns the original multiclass labels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"N5_Classification/image_data.csv\"\n",
    "labels_path = \"N5_Classification/image_labels.csv\"\n",
    "\n",
    "def load_data(binary_labels=False):\n",
    "    X = pd.read_csv(features_path, header=None).to_numpy()\n",
    "    y = pd.read_csv(labels_path, header=None).to_numpy().reshape(-1,)\n",
    "    \n",
    "    # select first 5 features\n",
    "    X = X[:,:5]\n",
    "    \n",
    "    # convert labels to (new) binary labels\n",
    "    # label for class 0 is y=1 and label for class 1 and class 2 is y=0.\n",
    "    if binary_labels:\n",
    "        y = (y == 0).astype(int)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function, we load the data and calculate the number of data points per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (178, 5) \n",
      "Shape of label vector: (178,)\n",
      "Number of samples from Class 0: 59\n",
      "Number of samples from Class 1: 71\n",
      "Number of samples from Class 2: 48\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "X, y = load_data()\n",
    "\n",
    "# Print information of dataset\n",
    "print(f\"Shape of feature matrix: {X.shape} \\nShape of label vector: {y.shape}\")\n",
    "print(f\"Number of samples from Class 0: {sum(y == 0)}\")\n",
    "print(f\"Number of samples from Class 1: {sum(y == 1)}\")\n",
    "print(f\"Number of samples from Class 2: {sum(y == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and Labels \n",
    "\n",
    "Our goal is to classify an area based on features of an aerial photograph of that area. The $i$-th area is characterized by the features $x^{(i)}_{1},\\ldots,x^{(i)}_{5}$ which we collect into the **feature vector** $\\mathbf{x}^{(i)} = \\big(x_{1}^{(i)},x_{2}^{(i)}, ... x_{5}^{(i)} \\big)^{T} \\in \\mathbb{R}^{5}$. It will be convenient to stack the feature vectors $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{5}$, obtained for all data points $i=1,\\dots,m$, into the feature matrix \n",
    "\n",
    "<a id='xm'></a>\n",
    "\\begin{equation*}\n",
    "    \\mathbf{X} = \\big(\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(178)}\\big)^T=\\begin{bmatrix}\n",
    "    x^{(1)}_{1}  & \\dots & x^{(1)}_{5} \\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    x^{(178)}_{1} & \\dots & x^{(178)}_{5}\n",
    "    \\end{bmatrix},\\ \\mathbf{X} \\in \\mathbb{R}^{m \\times n},\\ \\text{where } m=178, n=5.\n",
    "    \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "Besides its features $\\mathbf{x}^{(i)}$, the $i$-th area is characterized by the category $y^{(i)} \\in \\{0,1,2\\}$ which has been determined by a human expert. In principle, we could directly use the category $y^{(i)}$ as the label or quantity of interest. However, we will first consider the simpler binary classification problem, in which we attempt to predict whether a certain area has no pedestrian crossings or one or more pedestrian crossings. To this end, we define new labels with $y^{(i)}=1$ for areas with **no pedestrian crossings** and $y^{(i)}=0$ for areas with **one or more crossings**.\n",
    "\n",
    "It will be convenient to collect the labels of all images into the label vector \n",
    "\n",
    "<a id='vy'></a>\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y}=\\big(y^{(1)},y^{(2)},\\ldots,y^{(m)} \\big)^{T} = \\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots\\\\\n",
    "    y^{(m)}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{m}.\n",
    "    \\tag{2}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a id='demoboundary'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    " Demo. Visualize Data Points.\n",
    "\n",
    "The code snippet below loads the features $\\mathbf{X}$ and labels $\\mathbf{y}$ of the images and visualizes the data points on a scatter plot with respect to the first two features. The data points with no pedestrian crossings (new binary label is $y^{(i)} = 1$) are represented by circles and the data points with one or more crossings ($y^{(i)} = 0$) are represented by crosses.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data(binary_labels=True)\n",
    "\n",
    "idx_1 = np.where(y == 1) # Indices of no pedestrian crossings images\n",
    "idx_2 = np.where(y == 0) # Indices of one or more crossings images\n",
    "\n",
    "# Plot scatterplot of dataset with different markings for classes\n",
    "fig, axes = plt.subplots(figsize=(10, 6))\n",
    "axes.scatter(X[idx_1, 0], X[idx_1, 1], c='green', marker ='o', label='y =1; No crossings')\n",
    "axes.scatter(X[idx_2, 0], X[idx_2, 1], c='brown', marker ='x', label='y =0; One or more crossings ')\n",
    "\n",
    "# Set axis labels and legend\n",
    "axes.legend(loc='upper left', fontsize=12)\n",
    "axes.set_xlabel('feature $x_1$', fontsize=16)\n",
    "axes.set_ylabel('feature $x_2$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Logistic Regression\n",
    "\n",
    "<img src=\"N5_Classification/Log_Reg.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Logistic regression is a classification algorithm that uses a linear function to classify data points into distinct categories. With simplicity in mind, we present the theory of  logistic regression in the case of a **binary** classification problem, in which the label only takes on values 0 or 1. Later in the notebook, we will consider extending the binary model to multiclass classification via a **one-versus-rest** scheme.\n",
    "\n",
    "Logistic regression is a **probabilistic model** in the sense that we do not assume that the label $y$ of a data point is deterministically defined by the features $\\mathbf{x}$. Instead, we assume that the features $\\mathbf{x}$ are related to the probabilities of the data point having the different labels. The fundamental assumption of (binary) logistic regression is that the log-odds of a data point $\\mathbf{x}$ having label $y=1$ are linearly related to the features of the data point:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln \\frac{P(y=1)}{P(y=0)} = \\ln \\frac{P(y=1)}{1-P(y=1)} = w_0 + \\mathbf{w}^T \\mathbf{x} = w_0 + \\sum_{i=1}^n w_i x_i\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Here, the coefficient vector $\\mathbf{w} = \\big(w_1, w_2, \\ldots, w_n \\big)^T$ and intercept $w_0$ fully define the linear relationship between the features and the log-odds.\n",
    "\n",
    "Observe that $w_0 + \\mathbf{w}^T\\mathbf{x} > 0$ when the data point is more likely to have the label $y=1$, and $w_0 + \\mathbf{w}^T\\mathbf{x} < 0$ when it is more likely that the label $y=0$. Consequently, the sign of the function $h(\\mathbf{x}) = w_0 + \\mathbf{w}^T \\mathbf{x}$ can be used to predict the label $y$ of data points. The predicted labels outputted by the logistic regression model are\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{y} = \\begin{cases} 1 & \\mbox{ for } w_0 + \\mathbf{w}^T\\mathbf{x} \\geq 0 \\\\ 0 & \\mbox{ for } w_0 + \\mathbf{w}^T\\mathbf{x} < 0 \\end{cases}\n",
    " \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Given the model parameters, it is also relatively straightforward to calculate the estimated probabilities of the data point $\\mathbf{x}$ belonging to each class. To calculate the probability of the data point having the label $y=1$ we solve for $P(y=1)$ in equation (3) to obtain\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=1) = \\frac{1}{1 + \\exp(-w_0 -\\mathbf{w}^T \\mathbf{x})}\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "Since the label can only take the values 0 and 1, we obtain $P(y=0)$ as the probability of the complement of $P(y=1)$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=0) = 1 - P(y=1) = 1 - \\frac{1}{1 + \\exp(-w_0 -\\mathbf{w}^T \\mathbf{x})} = \\frac{1}{1 + \\exp(w_0 + \\mathbf{w}^T \\mathbf{x})}\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    " Fitting a logistic regression model\n",
    "\n",
    "When fitting a classification model, the goal is to find the parametrization of the model that results in the best predictions on the training set on average, as measured by some **loss function**. In classification problems we wish to maximize the **accuracy** of the classifier - the fraction of correctly predicted labels out of all data points. A seemingly natural choice of loss function in this case is the 0-1 loss, which incurs a loss of 1 if the prediction does not match the true label of a data point and 0 otherwise. This is due to the fact that minimizing the average 0-1 loss on the training set is equivalent to maximizing the accuracy. However, the average 0-1 loss cannot be minimized efficiently and is thus unusable in many applications. Most classification models consequently use some alternative loss function to optimize the classifier with respect to the model parameters.\n",
    "\n",
    "A logistic regression model is fitted by minimizing the average **logistic loss**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{E}(\\mathbf{w}) &=(1/m) \\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(P(y=1)\\big)-(1-y^{(i)})\\ln\\big(P(y=0)\\big) \\big] \\\\ &= (1/m) \\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\ln\\big(1-\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big) \\big]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "with respect to $\\mathbf{w}$ and $w_0$. Here, the **logistic loss** of a single data point $\\mathbf{x}^{(i)}$ with label $y^{(i)}$ is \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\mathbf{x}^{(i)},y^{(i)}) = -y^{(i)} \\ln \\big(\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\ln\\big(1-\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma(\\mathbf{z})$ is the **sigmoid function**\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp{(\\mathbf{-z})}}.\n",
    "\\end{equation}\n",
    "\n",
    "A central feature of the binary logistic loss is that only one of the terms in the loss is non-zero for any data point. If $y=1$ the second term is multiplied by 0, and similarly, the first term is multiplied by zero if $y=1$. Hence, the shape of the loss function for a given data point is determined by its label.\n",
    "\n",
    "It turns out that the average logistic loss is differentiable and convex and can thus be efficiently minimized using gradient based optimization algorithms, the principles of which are outside the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=4>Summary. Logistic Regression</font></b>\n",
    "\n",
    "* Linear functions can be used in classification task\n",
    "* One of the popular linear classifier is logistic regression\n",
    "* For linear predictor $h(\\mathbf{x})= w_0 + \\mathbf{w}^{T} \\mathbf{x}$, we can assign a class label based on the sign of  $h(\\mathbf{x})$: if $h(\\mathbf{x})$ > 0 the assigned class is 1 and if $h(\\mathbf{x})$ < 0 the assigned class is 0.\n",
    "* We want to choose such a weight vector $\\mathbf{w}$ that maximizes the probability (or likelihood) of the labels belonging to a specific class\n",
    "* Maximizing this probability is similar to minimizing the logistic loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"N5_Classification/logreg1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    " Demo. Logistic Loss.\n",
    "\n",
    "In this demo, we take a closer look at the logistic loss function and explore how its shape is affected by the label $y$ of a datapoint. To this end, we plot the logistic loss \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\big((\\mathbf{x},y);\\mathbf{w}\\big) = -y\\ln\\big(\\sigma\\big( \\mathbf{w}^{T} \\mathbf{x} \\big)\\big)-(1-y)\\ln\\big(1-\\sigma\\big(\\mathbf{w}^{T}\\mathbf{x}\\big) \\big)\n",
    "\\end{equation}\n",
    "      \n",
    "for both $y=1$ and $y=0$ as a function of the predictor value $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Recall that the value $\\mathbf{w}^{T} \\mathbf{x}$ gives the estimated log-odds of the true label $y$ being equal to $1$. Thus, large positive values of $\\mathbf{w}^T \\mathbf{x}$ represent a high degree of confidence in that $y=1$ and large negative values a high degree of confidence in that $y=0$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define sigmoid function according to formula (5)\n",
    "def sigmoid_func(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Choose values (w^T*x) for calculating loss \n",
    "range_x = np.arange(-5 , 5 , 0.01)\n",
    "\n",
    "# Calculate logistic loss for y=1 and y=0\n",
    "logloss_y1 = -np.log(sigmoid_func(range_x))\n",
    "logloss_y0 = -np.log(1-sigmoid_func(range_x))\n",
    "\n",
    "# Set fontsizes for matplotlib\n",
    "plt.rc('legend', fontsize=16) \n",
    "plt.rc('axes', labelsize=16) \n",
    "plt.rc('xtick', labelsize=12) \n",
    "plt.rc('ytick', labelsize=12) \n",
    "     \n",
    "# Plot the results, using the plot function in matplotlib.pyplot.\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6)) \n",
    "axes.plot(range_x, logloss_y1, linestyle=':', label=r'$y=1$', linewidth=3.0)\n",
    "axes.plot(range_x, logloss_y0, label=r'$y=0$', linewidth=2.0)\n",
    "\n",
    "# Set axis labels and title\n",
    "axes.set_xlabel(r'$\\mathbf{w}^{T}\\mathbf{x}$')\n",
    "axes.set_ylabel(r'$\\mathcal{L}((y,\\mathbf{x});\\mathbf{w})$')\n",
    "axes.set_title(\"logistic loss\", fontsize=20)\n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the logistic loss for data points with the label $y=1$ (true label) is **decreasing** with respect to the predicted log-odds $\\mathbf{w}^{T}\\mathbf{x}$. This indicates that the loss is decreasing with an increasing confidence in $y=1$. In contrast, the loss for data points with the label $y=0$ is **increasing** with respect to the predicted log-odds, which means that lower log-odds are associated with a lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing data for logistic regression\n",
    "\n",
    "When the features $\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n$ of the dataset are of very different scales, the iterative optimization algorithms used for training the logistic regression model (and many other models) are often slow to find the optimal parameters, or at times, might even fail to do so.\n",
    "\n",
    "In order to avoid these issues, it is common practice to **standardize** the dataset before using it to train the model. The feature matrix $\\mathbf{X}$ is standardized feature-wise - that is, for all elements in each column, we subtract the column mean and divide by the standard deviation ($\\sigma$) of the column to obtain the standard score:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_j^{(i)} = \\frac{\\mathbf{x}_j^{(i)} - \\bar{\\mathbf{x}}}{\\sigma(\\mathbf{x})}.\n",
    "\\end{equation}\n",
    "\n",
    "The result of this transformation is that the mean of each feature is 0, and the new values of the feature represent the deviation from the feature mean as measured by the number of standard deviations (+/-). Standardization can be performed in Scikit-learn in multiple ways, but the `StandardScaler` class is often convenient in practice.\n",
    "\n",
    " Standardization and model validation\n",
    "\n",
    "When training models with the intention of validating them, it is a common pitfall to standardize the entire dataset before splitting the data or performing cross-validation. By doing this, the information in the validation set is used to transform the training and validation data. This is non-realistic since the validation set is supposed to represent real, new data. After all, you cannot use data you do not possess to calculate the means and standard deviations used to transform the training data!\n",
    "\n",
    "The correct approach is to split the data before standardizing the training set. After the model has been trained on the standardized training data, we validate the model by standardizing the validation set **using the feature means and standard deviations from the training set**, and then calculating the validation error based on the standardized validation set. In cross-validation, this process is repeated for each split of the data.\n",
    "\n",
    "Scikit-learn comes with a very useful tool for sequentially combining different estimators called a pipeline estimator. The pipeline estimator consists of multiple steps, of which all but the final one must be Scikit-learn transformers. The final step is typically a classifier or regressor. \n",
    "\n",
    "A pipeline is built by instantiating a `Pipeline` object with the command `Pipeline(steps)`, where `steps` is a list of tuples, whose first argument is the name (of your choice) of the step and the second is an instance of a Scikit-learn transformer/estimator. For example, we could build a model that standardizes the data before performing logistic regression by creating the object \n",
    "\n",
    "`pipe = Pipeline([('scaler', StandardScaler()), ('log_reg', LogisticRegression())])`\n",
    "\n",
    "Now, the object in `pipe` has a similar interface as the final estimator in the pipeline. By calling `pipe.fit(X_train,y_train)`, the model sequentially fits all its steps by utilizing the output of each step as the input for the next one. In our example, the model would first fit the `StandardScaler` on the feature matrix `X_train`, and then the logistic regression model using the standardized feature matrix outputted by `StandardScaler.transform(X_train)` and the label vector `y_train`. \n",
    "\n",
    "When later using the model to, e.g., make predictions using `pipe.predict(X_test)`, the data flows sequentially through the same steps. First, `X_test` is standardized using the mean and standard deviation of `X_train` that was used to fit the scaler, and then, the transformed data used to predict the labels of `X_test` by using `LogisticRegression.predict(X_test)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logisticregression'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    " Student Task. Logistic Regression. \n",
    "\n",
    "In this task, you will train and validate a logistic regression model using Scikit-Learn. Due to the issues regarding standardization discussed above, you will use a `Pipeline` model containing a `StandardScaler` and a `LogisticRegression` object for this.\n",
    "    \n",
    "In slightly more detail, the steps you need to take to solve this task are as follows:\n",
    "    \n",
    "- Create a `Standard_Scaler` and a `LogisticRegression` object. For the logistic regression, remember to define the parameter `C=1e6`. The parameter `C` represents the inverse regularization strength. By setting this to a very large value, we obtain a Logistic Regression model that is not regularized.\n",
    "\n",
    "    \n",
    "- Create the pipeline with `StandardScaler` and `LogisticRegression` as its steps using the previously created objects. \n",
    "\n",
    "    \n",
    "- Fit the `Pipeline` object to training set.\n",
    "    \n",
    "    \n",
    "- Use the function `Pipeline.score()` to calculate the training and test accuracy for your classification model and store these in `acc_train` and `acc_test` respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the features and labels\n",
    "X, y = load_data(binary_labels=True)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\n",
    "\n",
    "### BEGIN STUDENT TASK ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print training and validation errors\n",
    "print(f\"Training accuracy: {acc_train}\")\n",
    "print(f\"Test accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the results\n",
    "assert acc_train > 0.95, \"Training accuracy is too low!\"\n",
    "assert acc_test < 0.89, \"Test accuracy is too high!\"\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what we learned last week, we can say that the logistic regression mildly overfits the training data. This claim is based on the training accuracy being significantly higher than the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Boundary of Logistic Regression \n",
    "\n",
    "After we have fitted the logistic regression model and found the optimal model parameters $\\mathbf{w}_{\\rm opt}$ and $w_0$, we can predict the label of any new image with the feature vector $\\mathbf{x} \\in \\mathbb{R}^n$ by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\begin{cases} 1 & \\mbox{ for } w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x} \\geq 0 \\\\ 0 & \\mbox{ for } w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x} < 0 \\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "The inequality $w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x} \\geq 0$ defines a closed half-space in $\\mathbb{R}^n$, and the inequality $w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x} < 0$ respectively defines an open half-space. These are separated by the hyperplane $w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x} = 0$ that is called the **decision boundary** of the logistic regression classifier. By recalling that $w_0 + \\mathbf{w}_{\\rm opt}^T \\mathbf{x}$ predicts the log-odds of the event $y=1$ in the logistic regression model, the decision boundary is naturally interpreted as the hyperplane on which the predicted odds for the labels are 1:1 - i.e. both labels are equally likely.\n",
    "\n",
    "In two-dimensional space (e.g., in $\\mathbb{R}^2$), the decision boundary is a line, whereas it is a plane in three-dimensional space. The hyperplane does not have an intuitive interpretation in higher dimensional space but still divides the entire space into two half-spaces.\n",
    "\n",
    "For most training data, the decision boundary determined by logistic regression will not perfectly separate the training data points according to $y^{(i)}=1$ and $y^{(i)}=0$. Thus, we typically have training samples with the same true label that are on opposite sides of the decision boundary. However, the decision boundary will be chosen such that one class dominates on each side.\n",
    "\n",
    "The decision boundary provides also a geometric interpretation of the magnitude $|w_0 + \\mathbf{w}_{\\rm opt}^{T} \\mathbf{x}|$ as the normal distance of a data point with features $\\mathbf{x}$ to the decision boundary. Thus, the larger $|w_0 + \\mathbf{w}_{\\rm opt}^{T} \\mathbf{x}|$, the farther away is the data point from the decision boundary and, in turn, the more reliable is the predicted label $\\hat{y}$ for this data point. On the other hand, if $|w_0 + \\mathbf{w}_{\\rm opt}^{T} \\mathbf{x}| \\approx 0$, then the data point with features $\\mathbf{x}$ is close to the decision boundary, i.e., it is a border case which cannot be classified reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. Linear Decision Boundary.\n",
    "\n",
    "In the code snippet below, we train a logistic regression model using only the first two features $x_{1}$ and $x_{2}$ of the images in the dataset. We then create two scatterplots containing the true and predicted labels respectively, as well as the decision boundary of the logistic regression classifier.\n",
    "    \n",
    "We can observe from the figure that while the decision boundary does not perfectly separate the data points with different true labels, the predicted labels are different on each side of the decision boundary, which is to be expected since the model effectively uses the boundary to predict the labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the features and labels\n",
    "X, y = load_data(binary_labels=True) \n",
    "\n",
    "# Use only the first two features\n",
    "X = X[:,:2]  \n",
    "\n",
    "# Create and fit logistic regression \n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Get the weights of the fitted model\n",
    "w0 = log_reg.intercept_\n",
    "w = log_reg.coef_ \n",
    "w = w.reshape(-1)\n",
    "\n",
    "# Calculate predictions\n",
    "y_pred = log_reg.predict(X)\n",
    "\n",
    "# Calculate the accuracy of predictions\n",
    "accuracy = log_reg.score(X, y)\n",
    "print(f\"Accuracy of classification: {round(100*accuracy, 2)}%\")\n",
    "\n",
    "# Minimum and maximum values of features x1 and x2\n",
    "x1_min, x2_min = np.min(X, axis=0)\n",
    "x1_max, x2_max = np.max(X, axis=0)\n",
    "\n",
    "# Compute x1, x2 values to plot the decision boundary h(x) = 0\n",
    "# for data with 2 features this means w1x1 + w2x2 + bias = 0 --> x2 = (-1/w2)*(w1x1+bias)\n",
    "x1_boundary = np.linspace(x1_min, x1_max, 100)\n",
    "x2_boundary = (-1/w[1])*(x1_boundary*w[0] + w0)\n",
    "\n",
    "options = [(\"Decision boundary and true labels\", y), \n",
    "           (\"Decision boundary and predicted labels\", y_pred)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 12))\n",
    "\n",
    "for i, (title, label) in enumerate(options):   \n",
    "    idx_1 = np.where(label == 1)[0] # index of each class 0 image (no crossings)\n",
    "    idx_2 = np.where(label == 0)[0] # index of each not class 0 image (one or two crossings)\n",
    "    # Plot datapoints\n",
    "    axes[i].scatter(X[idx_1, 0], X[idx_1, 1], marker='x', s=50, label=r'$c^{(i)}=0$')\n",
    "    axes[i].scatter(X[idx_2, 0], X[idx_2, 1], marker='o', s=50, label=r'$c^{(i)}\\in \\{1,2\\}$')\n",
    "    # Plot the decision boundary h(x) = 0\n",
    "    axes[i].plot(x1_boundary, x2_boundary, color='green', label=\"Decision boundary\")\n",
    "    # Set lables and axes' limits\n",
    "    axes[i].set_xlabel(r'$x_{1}$')\n",
    "    axes[i].set_ylabel(r'$x_{2}$')\n",
    "    axes[i].set_title(title, fontsize=16)\n",
    "    axes[i].set_xlim(x1_min-.5, x1_max+.5)\n",
    "    axes[i].set_ylim(x2_min-1, x2_max+1)\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Classification\n",
    "\n",
    "So far, we have considered the problem of classifying an image as $y=1$ if it belongs to \"class 0\" (no crossings) and as $y=0$ if not, i.e., if it belongs to \"class 1\" or \"class 2\" (one or two crossings). We have solved this binary classification problem using logistic regression. However, our ultimate goal is to classify an image according to all three categories of the images. \n",
    "\n",
    "There is a simple but useful trick for extending any binary classification method to handle more than two different label values or classes. The idea behind this trick, which is known as **one vs. rest**, is quite simple: just split the multiclass classification problem into several subproblems, each subproblem being one binary classification problem. We then apply a binary classification method (such as logistic regression) to each subproblem and combine their results to obtain a predicted label for the multiclass problem. \n",
    "\n",
    "For the image classification problem, using the three classes \"0\", \"1\" or \"2\", we obtain the following binary classification subproblems: \n",
    "\n",
    "- subproblem 0: classify samples into \"Class 0\" $(y=1)$ vs. \"not Class 0\" $(y=0)$  \n",
    "- subproblem 1: classify samples into \"Class 1\" $(y=1)$ vs. \"not Class 1\" $(y=0)$  \n",
    "- subproblem 2: classify samples into \"Class 2\" $(y=1)$ vs. \"not Class 2\" $(y=0)$\n",
    "\n",
    "Each subproblem amounts to testing if the image belongs to a particular class or not. The $k$th subproblem can be solved using logistic regression yielding a predictor $h^{(\\mathbf{w}_{k})}(\\mathbf{x})= (\\mathbf{w}_{k})^{T} \\mathbf{x}$. The predictor $h^{(\\mathbf{w}_{k})}(\\mathbf{x})= (\\mathbf{w}_{k})^{T} \\mathbf{x}$ indicates how likely the image belongs to the class $k$. We then assign the image to the class $k$ for which the confidence $h^{(\\mathbf{w}_{k})}(\\mathbf{x})$ is largest. \n",
    "\n",
    " Example\n",
    "\n",
    "Assume we want to classify a new data point (which is different from the $m$ data points in our dataset). To this end, we compute the feature vector $\\mathbf{x}=(x_{1},x_{2},...,x_{5})^{T}$ of this new data point and apply the three subproblem predictors, yielding the following prediction values: \n",
    "\n",
    "* subproblem 0: $h^{(\\mathbf{w}_{0})}(\\mathbf{x}) = 0.1$ (\"Class 0 vs. not Class 0\")\n",
    "* subproblem 1: $h^{(\\mathbf{w}_{1})}(\\mathbf{x}) = 0.4$ (\"Class 1 vs. not Class 1\") \n",
    "* subproblem 2: $h^{(\\mathbf{w}_{2})}(\\mathbf{x}) = 0.8$ (\"Class 2 vs. not Class 2\")\n",
    "\n",
    "From these results, we can see that the predictor $h^{(\\mathbf{w}_{2})}(\\mathbf{x})$ for sub problem 3 (`Class 2` vs. `not Class 2`) yields the highest confidence. Hence, we classify this new data point as `Class 2`. \n",
    "\n",
    "<img src=\"N5_Classification/Regression_Zebra.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    " Demo. Multiclass Classification.\n",
    "\n",
    "The code snippet below illustrates how multiclass classification via logistic regression can be applied using scikit-learn. By defining the parameter `multi_class='ovr'` when initializing the `LogisticRegression` model, we make the logistic regression model use the one vs. rest scheme described above to perform multiclass classification.\n",
    "\n",
    "We also use the function `LogisticRegression.predict_proba(X)` to obtain the predicted probabilities of the data points having each label. The function returns the probabilities in an array of shape `(m, n_labels)`, where `m` is the number of data points and `n_labels` the number of labels in the classification problem. That is, the $i$:th row of the array contains the predicted probabilities of the $i$:th data point having each label.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load features and labels with all labels\n",
    "X, y = load_data()\n",
    "\n",
    "# Fit logistic regression model\n",
    "log_reg = LogisticRegression(multi_class='ovr')\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Predict labels and probabilities\n",
    "y_pred = log_reg.predict(X)\n",
    "pred_probabilities = log_reg.predict_proba(X)\n",
    "\n",
    "print(f\"Labels: {set(y_pred)}\")\n",
    "print(f\"Predicted probability of each label for the first data point: {pred_probabilities[0]}\")\n",
    "print(f\"Predicted label for the first data point: {y_pred[0]}\")\n",
    "print(f\"True label for the first data point: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output that, for the first datapoint, the probabilities for the labels $y=0,1,2$ are approximately $0.92$, $0.004$, and $0.08$ respectively. As such, the model predicts $y=0$ with high confidence.\n",
    "\n",
    "\n",
    " Regularization in logistic regression\n",
    "\n",
    "Recall from the previous round that regularization is a technique with which we aim to improve the performance of our trained model on new data by minimizing a penalized average loss function. More specifically, we add a penalty term to the average loss that penalizes complex predictors more than simpler ones. The idea is that complex predictors are more likely to overfit the training data. \n",
    "\n",
    "In principle, regularization is performed the same way for logistic regression as in the Ridge and Lasso model considered last week. The difference is that the average logistic loss replaces the MSE in penalized loss function. A such, the function to minimize is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{E}(\\mathbf{w}) &= (1/m) \\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\ln\\big(1-\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big) \\big] + \\alpha \\mathcal{R}(\\mathbf{w}),\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha \\mathcal{R}(\\mathbf{w})$ is the penalty term. The regularization terms are generally of the same form as those considered in Notebook 4. For example, the default regularization term in Scikit-learn's logistic regression is $\\mathcal{R}(\\mathbf{w})=\\|\\mathbf{w}\\|_2^2$.\n",
    "\n",
    "In practice, the logistic regression model in Scikit-learn uses an inverse regularization coefficient $C$ instead of $\\alpha$. Whereas $\\alpha$ is used to control the magnitude of $\\mathcal{R}(\\mathbf{w})$ in the regularized loss function,  $C$ controls the weight of the average logistic loss instead:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{E}(\\mathbf{w}) &= (C/m) \\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\ln\\big(1-\\sigma(w_0 + \\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big) \\big] + \\mathcal{R}(\\mathbf{w}).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "As such, small values of $C$ correspond to a heavily regularized model, while large values lead to weaker regularization. Next, we will attempt to find a good logistic regression model for the multiclass problem by tuning the inverse regularization strength $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logisticregressiontuning'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    " Student Task. Tuning a Logistic Regression model. \n",
    "\n",
    "In this task, you will tune the inverse regularization strength $C$ of the logistic regression model to find the value that minimizes the validation error on the multi-class classification problem. To this end, you will use the class `GridSearchCV`, with which you familiarized yourself in the previous round.\n",
    "    \n",
    "Recall that the `GridSearchCV` class takes as arguments the model to be tuned and a dictionary of key-value pairs, where the key is the name of a tunable parameter and the value a list of candidate values for that parameter.\n",
    "    \n",
    "In this task, the model is a `Pipeline` estimator similar to the one you have used in \"Student Task. Logistic Regression\". When the model in `GridSearchCV` is a `Pipeline` object, the parameters in the parameter dictionary are named as `{step_name}__{parameter_name}`, where `{step_name}` is the name of the pipeline step whose parameter is tuned and `{parameter_name}` is the name of the parameter. Note **double underscore** in `{step_name}__{parameter_name}`\n",
    "    \n",
    "For example, if the model is \n",
    "    \n",
    "`pipe = Pipeline([('scaler', StandardScaler()), ('log_reg', LogisticRegression(multi_class='ovr'))])`\n",
    "    \n",
    "the name of the parameter `C` in the parameter dictionary of `GridSearchCV` object is `log_reg__C`.\n",
    "    \n",
    "The steps you have to perform to solve this task are as follows:\n",
    "    \n",
    "- Create the `Pipeline` whose steps are standardization and logistic regression. The logistic regression model should utilize one-versus-rest for multiclass classification (`multi_class='ovr'`).\n",
    "    \n",
    "    \n",
    "- Create a parameter dictionary containing one key-value pair containing the name and candidate values `C_candidates` of the parameter `C` of the logistic regression model. See above how the parameter is named when the logistic regression model is part of a `Pipeline`.\n",
    "    \n",
    "    \n",
    "- Create a `GridSearchCV` object with the `Pipeline` and the parameter dictionary as inputs. Also, pass the parameters `refit=True`, `cv=5` and `return_train_score=True` to the constructor. Then, perform the 5-fold cross-validation by calling the `GridSeachCV.fit(X_trainval,y_trainval)` function of your `GridSearchCV` object.\n",
    "    \n",
    "  \n",
    "- Store the average training and validation accuracies in the variables `acc_train` and `acc_val` respectively. Remember that these can be retrieved from the dictionary `GridSearchCV.cv_results_` by using the keys `mean_train_score` and `mean_test_score`. \n",
    "\n",
    "\n",
    "- Store the best model (i.e. model with the optimal `C`) in the variable `best_model`. The best model can be obtained from the attribute `GridSearchCV.best_estimator_`.\n",
    "    \n",
    "**Hint:** See [here](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html) for an example on combining `Pipeline` and `GridSearchCV`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Split dataset into train-val and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)\n",
    "\n",
    "# Candidates for the inverse regularization strength\n",
    "C_candidates = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "### STUDENT TASK ###\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# Print training and validation errors\n",
    "print(f\"Training accuracy: {acc_train}\")\n",
    "print(f\"Validation accuracy: {acc_val}\")\n",
    "print(f\"Best model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the results\n",
    "assert len(acc_train) == len(C_candidates), \"acc_train is of the wrong size!\"\n",
    "assert len(acc_val) == len(C_candidates), \"acc_val is of the wrong size!\"\n",
    "assert best_model.get_params()['log_reg__C'] == 1, \"The optimal parameter value is wrong!\"\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracies for the different values of C\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(C_candidates, acc_train, '-o', label='Training accuracy')\n",
    "plt.plot(C_candidates, acc_val, '-o',label='Validation accuracy')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title(\"Accuracy vs. inverse regularization strength\")\n",
    "plt.xlabel('Inverse regularization strength C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have completed the task correctly, you should see plot similar to this one:\n",
    "\n",
    "<img src=\"N5_Classification/gridsearch.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "\n",
    "Computing the accuracy as the fraction of correctly classified data points for which $\\hat{y}^{(i)}=y^{(i)}$ is only one possible way to check how well you did. In some applications, the accuracy score is not a very useful quality measure. In particular, this is true for applications where the different classes occur with significantly different frequencies (\"imbalanced data\"). A more fine-grained assessment of a classification method is provided by computing a confusion matrix. \n",
    "\n",
    "A [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix) is a matrix of shape `(n_labels, n_labels)`, in which the value of the element in row $i$ and column $j$ represents the number of data points with the true label corresponding to index $i$ that are predicted by the classifier as having the label corresponding to index $j$. As such, the elements along the diagonal ($i=j$) show the amount of correctly classified data points for each label.\n",
    "\n",
    "Using a confusion matrix, it is possible to analyze the classifier's performance for each possible value of the true label. For example, one might discover that the classifier performs particularly well or poorly on some specific classes.\n",
    "\n",
    "It is often convenient to calculate a so-called **normalized confusion matrix** instead of the standard one. In the normalized confusion matrix, the element in the $i$th row and $j$th column contains the proportion of data points with the true label corresponding to $i$ that are classified as having the label corresponding to $j$. The rows of this matrix sum up to 1. The normalization makes it easier to compare the classifier's performance for the different true labels when they occur with different frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logregconf'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    " Demo. Confusion Matrix. \n",
    "\n",
    "The confusion matrix is commonly visualized as an `n_labels x n_labels` grid of cells, where the cell colors are related to the number/proportion of data points in the corresponding element in the confusion matrix. This greatly enhances the confusion matrix's readability, especially when the number of labels (and the size of the matrix) is large.\n",
    "    \n",
    "In scikit-learn, a confusion matrix can be visualized using the function [`sklearn.metrics.plot_confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn-metrics-plot-confusion-matrix). The required inputs to the function are a fitted classifier, the feature matrix `X`, and the label vector `y`. One can also define other parameters for the function that control the appearance and type of the plotted confusion matrix. The function returns the object `display`, where the confusion matrix can be accessed with `display.confusion_matrix`. \n",
    "    \n",
    "In the code snippet below, we use the `plot_confusion_matrix()` function to plot both, standard and normalized confusion matrix, based on **test set** for the multi-class logistic regression classifier saved in the previous task in a variable `best_model`. In addition to the required input parameters, we pass the desired class names for the confusion matrix in the parameter `display_labels`, the desired color map in the parameter `cmap`, as well as the `axes` object we wish to plot the confusion matrix on in the parameter `ax`. Furthermore, we pass a value in the parameter `normalize`, which indicates whether or not the confusion matrix should be normalized or not.\n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Define class labels for confusion matrices\n",
    "classes = ['Class 0','Class 1','Class 2']\n",
    "\n",
    "# Define plotting options (title, normalization, axes index)\n",
    "options = [(\"Confusion matrix\", None, 0),\n",
    "           (\"Normalized confusion matrix\", 'true', 1)]\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Create subplots (1 row, 2 columns)\n",
    "plt.rc('font', size=14)  # Set fontsize\n",
    "for title, normalize, ax_idx in options:\n",
    "    # main parameters of function `plot_confusion_matrix` are:\n",
    "    # trained classifier (best_model), data (X_test, y_test)\n",
    "    disp = plot_confusion_matrix(best_model, X_test, y_test,\n",
    "                                 display_labels=classes,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize, ax=axes[ax_idx])\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrices, we can see that our logistic regression model classifies data points with the true labels $y=0$ and $y=1$ pretty well, whereas accuracy for the last class is much lower. Observe that since the number of data points in each class is slightly different, the normalized confusion matrix makes it much easier to interpret the model's comparative performance on data points with different labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees\n",
    "\n",
    "We will learn another classification method that is referred to as **decision trees**. A decision tree is a flowchart-like representation of a predictor function $h(\\mathbf{x})$ that reads in the features $\\mathbf{x}$ of a data point and outputs a predicted label $\\hat{y}=h(\\mathbf{x})$. The decision tree consists of **nodes**, which represent certain tests, e.g., \"is the first feature $x_{1}$ larger than 10?\". The nodes are connected by **branches** that correspond to the result or outcome of a test (there is one outgoing branch for each possible answer of a test node). By following the branches, we end up at a leaf node (which has no further branches). Each leaf node is associated with a certain output value $h(\\mathbf{x})$. The picture below depicts a decision tree for wine classification with test nodes colored blue and leaf nodes colored orange and green. \n",
    "\n",
    "<img src=\"N5_Classification/Decision_Tree.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "Now, you might wonder how do we choose the test nodes? The basic idea is the same as in linear or logistic regression, we try out many different decision trees (using different choices of test nodes) and pick the one which results in the smallest average loss incurred on some labeled training data points $(\\mathbf{x}^{(i)},y^{(i)})$. However, in contrast to logistic regression, this learning or optimization problem involves searching over a discrete set of different configurations of test nodes instead of a continuous convex optimization of the model parameters. This makes training decision trees computationally more challenging than logistic regression that allows using efficient convex optimization methods (such as gradient-based methods). However, clever approaches to learning good decision trees with a reasonable amount of computational resources have been developed, although details of these are outside the scope of this course. \n",
    "\n",
    "Video on the basic concept of decision trees:\n",
    "\n",
    "- https://www.youtube.com/watch?v=9w16p4QmkAI\n",
    "\n",
    "If you want to learn more details about decision trees, beyond the requirements of this course, we refer you to: \n",
    "\n",
    "- https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "- https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\n",
    "- https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demotreeboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "### Demo. Decision Boundary of a Decision Tree.\n",
    "\n",
    "In the code below, we train a decision tree on the first two features of the dataset, and plot the decision boundary of the resulting classifier in a scatter plot along with the data. The data points with $y^{(i)} = 1$ are indicated by \"x\" while all samples with true label $y^{(i)} =0$ are indicated by \"o\".\n",
    "\n",
    "The algorithms that are used to train a decision tree are not affected by variables of different scales. As such, we do not need to standardize the data using pipelines when using decision tree models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, Y, cmap='Paired_r'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function with which to plot decision boundary\n",
    "    \"\"\"\n",
    "    \n",
    "    # step size\n",
    "    h = 0.02\n",
    "    # min-max values of features x1 and x2\n",
    "    x1_min, x1_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
    "    x2_min, x2_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
    "    # x1, x2 values for plotting \n",
    "    x1 = np.arange(x1_min, x1_max, h)\n",
    "    x2 = np.arange(x2_min, x2_max, h)\n",
    "    # create grid of x1,x2 values (all possible combinations of x1 and x2 values)\n",
    "    x1x1, x2x2 = np.meshgrid(x1, x2)\n",
    "    # get predictions for each x1,x2 pair\n",
    "    Z = clf.predict(np.c_[x1x1.ravel(), x2x2.ravel()])\n",
    "    Z = Z.reshape(x1x1.shape)\n",
    "    \n",
    "    idx_1 = np.where(Y == 1)[0] # index of each class 0 iamge.\n",
    "    idx_2 = np.where(Y == 0)[0] # index of each not class 0 image\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.contourf(x1x1, x2x2, Z, cmap=cmap, alpha=0.25)\n",
    "    plt.contour(x1x1, x2x2, Z, colors='k', linewidths=0.5)\n",
    "    plt.scatter(X[idx_1, 0], X[idx_1, 1], marker='x', label='class 0')\n",
    "    plt.scatter(X[idx_2, 0], X[idx_2, 1], marker='o', label='class 1', edgecolors='k')\n",
    "    plt.xlabel(r'Feature 1')\n",
    "    plt.ylabel(r'Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data and select only the first two features\n",
    "X, y = load_data(binary_labels=True)\n",
    "X = X[:,:2]\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='entropy')   # define object \"clf\" which represents a decision tree\n",
    "clf.fit(X, y)                    # learn a decision tree that fits well the labeled images  \n",
    "y_pred = clf.predict(X)          # compute the predicted labels for the images\n",
    "\n",
    "# Calculate the accuracy score of the predictions\n",
    "accuracy = clf.score(X, y)\n",
    "print(f\"Accuracy: {round(100*accuracy, 2)}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(clf, X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is perfectly accurate, but the decision function looks quite peculiar. Do you think that this model would generalize well to new data?\n",
    "\n",
    "Next, you will train a decision tree and calculate the training and validation errors to assess the generalization capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtclassifier'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    " Student Task. Decision Tree Classifier.\n",
    "    \n",
    "In this student task, you will train a decision tree classifier on a set of training data using Scikit-learn `DecisionTreeClassifier` class, and calculate the models training and test accuracies. The accuracies are stored in the variables `acc_train` and `acc_test` respectively. \\\n",
    "When creating the decision tree object you should set the parameter `criterion='entropy'`. The argument `criterion` corresponds to a particular choice for the loss function to be used. For background information consult the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "    \n",
    "The process of training a decision tree model is similar to that of other classifiers (and regressors) in scikit-learn, so you will probably not need extra details this time! \n",
    "    \n",
    "**Hint**: the accuracy can by easily calculated by using the `.score` function of the decision tree object. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data to feature matrix X and label vector y \n",
    "X, y = load_data(binary_labels=True)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\n",
    "\n",
    "### STUDENT TASK ###\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print training and validation error\n",
    "print(f\"Training error: {acc_train}\")\n",
    "print(f\"Test error: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the outputs\n",
    "assert acc_train > 0.99, \"Training accuracy is too low.\"\n",
    "assert acc_test > 0.9, \"Test accuracy is too low.\"\n",
    "assert acc_test < 1, \"Test accuracy is too high.\"\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trained decision tree severely overfits the training data. While the accuracy of the model is perfect on the training data, it performs worse on the test set. This is in fact very common when using decision trees, and should be addressed by tuning some of the hyperparameters of the decision tree to obtain a model with better generalization capability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtvis'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    " Demo. Visualizing the decision tree. \n",
    "    \n",
    "One of the advantages of the decision tree classifier is its interpretability. In the code snippet below, we visualize the fitted decision tree using the `plot_tree()` function from the `tree` module in scikit-learn.\n",
    "    \n",
    "In each node of the tree, the top row describes the decision rule of the cell. For instance, the root node splits the dataset based on whether the value of the 1st feature is larger or smaller than $12.905$. The `samples` field indicates how many data points are allocated to that particular node, and the `value` field indicates how these are distributed with respect to the true labels. The `class` field specifies the class to which the data points in the node are classified at that level of the tree. The leaf nodes of the tree define the final labels of the data points.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "feature_cols = [\"x\" + str(i) for i in range(len(X[0,:]))] # needed for visualization\n",
    "label_names = ['0', '1']\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(16,14))\n",
    "plot_tree(clf, filled=True, rounded=True, feature_names=feature_cols, class_names=label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between logistic regression and decision trees\n",
    "\n",
    "The two classification methods, logistic regression and decision trees, both aim at learning a good predictor $h(\\mathbf{x})$ which allows determining the label $y$ of the data point based on some features $\\mathbf{x}$. A major difference of the two models is the form of the predictor function $h(\\mathbf{x})$ they use. Logistic regression uses linear predictor functions $h(\\mathbf{x})=w_0 + \\mathbf{w}^{T} \\mathbf{x}$ (which are thresholded to get discrete label predictions $\\hat{y}$). \n",
    "\n",
    "In contrast to the linear functions used in logistic regression, decision trees use predictor functions that are obtained from flow charts (decision trees) consisting of various tests on the features $\\mathbf{x}$. Using sufficiently large decision trees allows to represent highly non-linear functions $h(\\mathbf{x})$. In particular, decision trees can perfectly separate data points (according to their labels), which cannot be separated by any straight line (which are the only possible decision boundaries for logistic regression). \n",
    "\n",
    "<table><tr>\n",
    "    <td><img src='N5_Classification/lr1.png' style=\"width: 500px;\"></td>\n",
    "    <td><img src='N5_Classification/tree1.png' style=\"width: 500px;\"></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left side - the green line is a decision boundary of logreg. The color of the dots represent true labels (blue or red) and the color of the background is classification according to logreg. The intensity of background color represents confidence in a given label (darker red/blue - high confidence, light red/blue - lower confidence, white - 50/50 chance).\n",
    "\n",
    "\n",
    "\n",
    "Right side - Depicted zones represent decision tree's flowchart/boundaries\n",
    "In the right plot, background color represents classification by the tree: red zones are zones where datapoints predicted label is \"red\": blue zones - predicted labels are \"blue\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><img src='N5_Classification/Screenshot 2022-02-21 at 11.07.36.png' style=\"width: 500px;\"></td>\n",
    "    <td><img src='N5_Classification/image (8).png' style=\"width: 500px;\"></td>\n",
    "</tr></table>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QuestionN5_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Question N5.1. \n",
    "\n",
    "<p>How many features can be used for logistic regression?</p>\n",
    "\n",
    "<ol>\n",
    "  <li>None</li>\n",
    "  <li>One (1)</li>\n",
    "  <li>Thirteen (13)</li>\n",
    "  <li>Any number of features (given enough computational resources)</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "#answer_N5_Q1  =.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_N5_Q1 in [1,2,3,4], '\"answer_N5_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QuestionN5_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Question N5.2. \n",
    "\n",
    "<p>When performing logistic regression, we are trying to....</p>\n",
    "\n",
    "<ol>\n",
    "  <li>Solve a minimum likelihood problem.</li>\n",
    "  <li>Maximize the average logistic loss.</li>\n",
    "  <li>Minimize the average logistic loss.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "#answer_N5_Q2  = .\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_N5_Q2 in [1,2,3], '\"answer_N5_Q2\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QuestionN5_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Question N5.3. \n",
    "\n",
    "<p>Consider an arbitrary set of $m$ labeled data points having two features $\\mathbf{x}^{(i)} \\in \\big(x^{(i)}_{1},x^{(i)}_{2}\\big)^{T}$ and a binary label $y^{(i)} \\in \\{0,1\\}$. How large can the sample size $m$ be such that we can for sure always find a straight line such that all points $\\mathbf{x}^{(i)}$ with the same label $y^{(i)}$ lie on the same side (but not on top) of the line. </p>\n",
    "\n",
    "<ol>\n",
    "  <li>$m \\leq 2$</li>\n",
    "  <li>$m = 3$</li>\n",
    "  <li>$m = 4$</li>\n",
    "  <li>$m = 6$</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "#answer_N5_Q3  = .\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_N5_Q3 in [1,2,3,4], '\"answer_N5_Q3\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
